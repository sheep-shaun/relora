{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.relora import optimizer_reset, get_cosine_schedule_with_multiple_warmups\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_path = \"../data\"\n",
    "models_path = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:01.376482Z",
     "iopub.status.busy": "2024-06-02T21:01:01.375920Z",
     "iopub.status.idle": "2024-06-02T21:01:01.381926Z",
     "shell.execute_reply": "2024-06-02T21:01:01.381428Z",
     "shell.execute_reply.started": "2024-06-02T21:01:01.376461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:01.383018Z",
     "iopub.status.busy": "2024-06-02T21:01:01.382569Z",
     "iopub.status.idle": "2024-06-02T21:01:01.386103Z",
     "shell.execute_reply": "2024-06-02T21:01:01.385658Z",
     "shell.execute_reply.started": "2024-06-02T21:01:01.382999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "global_batch_size = 512\n",
    "accumulation_steps = global_batch_size // batch_size\n",
    "\n",
    "learning_rate = 3e-4\n",
    "betas = (0.9, 0.95)\n",
    "eps = 1e-8\n",
    "gradient_clipping = 1.0\n",
    "weight_decay = 0.1\n",
    "\n",
    "adjust_warmup_iters = 250  # from warm model\n",
    "first_warmup_iters = 100\n",
    "restart_warmup_iters = 50\n",
    "min_lr_ratio = 0.001\n",
    "\n",
    "adjust_train_iters = 500  # from warm model\n",
    "train_iters = 2000  # train_iters including adjust_train_iters\n",
    "eval_save_interval = 50\n",
    "val_iters = 20\n",
    "\n",
    "lora_rank = 128\n",
    "lora_dropout = 0.1\n",
    "lora_alpha = 32\n",
    "relora_steps = 500\n",
    "reset_optimizer_on_relora = False\n",
    "optimizer_magnitude_pruning = 0.8\n",
    "\n",
    "model_name = os.path.join(models_path, \"pythia-14m_500_of_2000\")\n",
    "tokenizer_name = \"EleutherAI/pythia-14m\"\n",
    "tokenizer_revision = \"step0\"\n",
    "\n",
    "dataset_path = \"allenai/c4\"\n",
    "dataset_name = \"realnewslike\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:01.387560Z",
     "iopub.status.busy": "2024-06-02T21:01:01.387394Z",
     "iopub.status.idle": "2024-06-02T21:01:12.888299Z",
     "shell.execute_reply": "2024-06-02T21:01:12.887759Z",
     "shell.execute_reply.started": "2024-06-02T21:01:01.387543Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e74f46ee19749fa9211c7e76fa0979e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accf4ff5dfa348c09c96f5dd8ee89129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c2dcafd3724dcfacd2456aeb18e562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, dataset_name)\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:12.889263Z",
     "iopub.status.busy": "2024-06-02T21:01:12.888985Z",
     "iopub.status.idle": "2024-06-02T21:01:12.891906Z",
     "shell.execute_reply": "2024-06-02T21:01:12.891457Z",
     "shell.execute_reply.started": "2024-06-02T21:01:12.889243Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, revision=tokenizer_revision)\n",
    "\n",
    "def tokenize(data):\n",
    "    outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = list()\n",
    "    # deleting samples shorter than context_length tokens\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:07:44.222661Z",
     "iopub.status.busy": "2024-06-02T21:07:44.222486Z",
     "iopub.status.idle": "2024-06-02T21:07:44.225462Z",
     "shell.execute_reply": "2024-06-02T21:07:44.225034Z",
     "shell.execute_reply.started": "2024-06-02T21:07:44.222641Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ed48a0d70a4a0ba59ede6f9347615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(data_path, \"train_dataset\")):\n",
    "    train_dataset = Dataset.load_from_disk(os.path.join(data_path, \"train_dataset\"))\n",
    "else:\n",
    "    train_dataset = dataset[\"train\"].map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    train_dataset.save_to_disk(os.path.join(data_path, \"train_dataset\"))\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(data_path, \"val_dataset\")):\n",
    "    val_dataset = Dataset.load_from_disk(os.path.join(data_path, \"val_dataset\"))\n",
    "else:\n",
    "    val_dataset = dataset[\"validation\"].map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"validation\"].column_names\n",
    "    )\n",
    "    val_dataset.save_to_disk(os.path.join(data_path, \"val_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.select(range(len(val_dataset) // (batch_size * val_iters) * (batch_size * val_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:07:56.497721Z",
     "iopub.status.busy": "2024-06-02T21:07:56.497152Z",
     "iopub.status.idle": "2024-06-02T21:07:56.501566Z",
     "shell.execute_reply": "2024-06-02T21:07:56.501092Z",
     "shell.execute_reply.started": "2024-06-02T21:07:56.497700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              worker_init_fn=seed_worker,\n",
    "                              generator=g)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            worker_init_fn=seed_worker,\n",
    "                            generator=g,\n",
    "                            drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLora Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:56:45.779339Z",
     "iopub.status.busy": "2024-06-02T21:56:45.778937Z",
     "iopub.status.idle": "2024-06-02T21:56:45.782142Z",
     "shell.execute_reply": "2024-06-02T21:56:45.781664Z",
     "shell.execute_reply.started": "2024-06-02T21:56:45.779319Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=lora_rank, \n",
    "                         target_modules=[\"query_key_value\", \"dense\",\n",
    "                                         \"dense_h_to_4h\", \"dense_4h_to_h\"], \n",
    "                         lora_dropout=lora_dropout, \n",
    "                         lora_alpha=lora_alpha)\n",
    "\n",
    "# By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and zeros for weight B \n",
    "# resulting in an identity transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:31.507159Z",
     "iopub.status.busy": "2024-06-02T21:58:31.506522Z",
     "iopub.status.idle": "2024-06-02T21:58:32.054278Z",
     "shell.execute_reply": "2024-06-02T21:58:32.053731Z",
     "shell.execute_reply.started": "2024-06-02T21:58:31.507134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in GPTNeoXForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in GPTNeoXModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                          attn_implementation=\"flash_attention_2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:32.055442Z",
     "iopub.status.busy": "2024-06-02T21:58:32.055272Z",
     "iopub.status.idle": "2024-06-02T21:58:32.092967Z",
     "shell.execute_reply": "2024-06-02T21:58:32.092461Z",
     "shell.execute_reply.started": "2024-06-02T21:58:32.055425Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 15,640,576 || trainable%: 10.0563\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:32.093781Z",
     "iopub.status.busy": "2024-06-02T21:58:32.093618Z",
     "iopub.status.idle": "2024-06-02T21:58:32.097852Z",
     "shell.execute_reply": "2024-06-02T21:58:32.097393Z",
     "shell.execute_reply.started": "2024-06-02T21:58:32.093763Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainable_params = [p for p in lora_model.parameters() if p.requires_grad]\n",
    "lora_params = [p for n, p in lora_model.named_parameters() if p.requires_grad and \"lora_\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:34.132990Z",
     "iopub.status.busy": "2024-06-02T21:58:34.132535Z",
     "iopub.status.idle": "2024-06-02T21:58:34.136419Z",
     "shell.execute_reply": "2024-06-02T21:58:34.135933Z",
     "shell.execute_reply.started": "2024-06-02T21:58:34.132971Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "optimizer_state_keys = [\"exp_avg\", \"exp_avg_sq\"]\n",
    "\n",
    "scheduler = get_cosine_schedule_with_multiple_warmups(\n",
    "    optimizer,\n",
    "    num_training_steps=train_iters,\n",
    "    first_warmup_steps=first_warmup_iters,\n",
    "    restart_warmup_steps=restart_warmup_iters,\n",
    "    restart_every=relora_steps,\n",
    "    adjust_step=adjust_train_iters,\n",
    "    adjust_warmup_iters=adjust_train_iters,\n",
    "    min_lr_ratio=min_lr_ratio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:45.350100Z",
     "iopub.status.busy": "2024-06-02T21:58:45.349644Z",
     "iopub.status.idle": "2024-06-02T21:58:45.358667Z",
     "shell.execute_reply": "2024-06-02T21:58:45.358189Z",
     "shell.execute_reply.started": "2024-06-02T21:58:45.350081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "lora_model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "    lora_model, optimizer, train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, val_iters):\n",
    "    model.eval()\n",
    "    val_losses = list()\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            val_losses.append(outputs.loss.item())\n",
    "        if step + 1 >= val_iters:\n",
    "            break\n",
    "    val_loss = np.mean(val_losses)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:49.261706Z",
     "iopub.status.busy": "2024-06-02T21:58:49.261283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 202/6000 [00:22<21:10,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 50, 'loss/train': 6.148068511486054, 'loss/val': 6.164088487625122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 402/6000 [00:40<19:24,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 100, 'loss/train': 6.143584821224213, 'loss/val': 6.149805474281311}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 602/6000 [00:59<19:29,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 150, 'loss/train': 6.138176422119141, 'loss/val': 6.143473505973816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 802/6000 [01:18<18:43,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 200, 'loss/train': 6.136109302043915, 'loss/val': 6.15586485862732}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1002/6000 [01:36<17:40,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 250, 'loss/train': 6.12351722240448, 'loss/val': 6.134815359115601}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1202/6000 [01:55<17:08,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 300, 'loss/train': 6.112013220787048, 'loss/val': 6.130725860595703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1402/6000 [02:14<16:20,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 350, 'loss/train': 6.11193799495697, 'loss/val': 6.12102439403534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1602/6000 [02:32<15:32,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 400, 'loss/train': 6.098917257785797, 'loss/val': 6.102164483070373}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1802/6000 [02:51<14:57,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 450, 'loss/train': 6.096788954734802, 'loss/val': 6.129397511482239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2002/6000 [03:10<14:26,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 500, 'loss/train': 6.08454080581665, 'loss/val': 6.141530919075012}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2004/6000 [03:10<13:38,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing lora reset at update step 501. Current lr is 4.2219897192981734e-06\n",
      "LoRA reset took 0.05s\n",
      "Performing optimizer reset at update step 501. Current lr is 4.2219897192981734e-06\n",
      "Percent of optimizer states zeroed: 80.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2202/6000 [03:29<13:43,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 550, 'loss/train': 6.111590788364411, 'loss/val': 6.104821133613586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2402/6000 [03:48<13:20,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 600, 'loss/train': 6.108436465263367, 'loss/val': 6.1029904842376705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2602/6000 [04:07<12:15,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 650, 'loss/train': 6.100020468235016, 'loss/val': 6.126300239562989}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2802/6000 [04:26<11:21,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 700, 'loss/train': 6.103038260936737, 'loss/val': 6.105225682258606}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3002/6000 [04:45<10:58,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 750, 'loss/train': 6.099106926918029, 'loss/val': 6.110839772224426}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 3202/6000 [05:04<10:12,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 800, 'loss/train': 6.1043798756599426, 'loss/val': 6.096076250076294}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3402/6000 [05:23<09:27,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 850, 'loss/train': 6.09590485572815, 'loss/val': 6.107667398452759}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3602/6000 [05:42<08:32,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 900, 'loss/train': 6.091790170669555, 'loss/val': 6.1140090227127075}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3802/6000 [06:01<07:57,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 950, 'loss/train': 6.095734131336212, 'loss/val': 6.111231660842895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4002/6000 [06:20<07:18,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 1000, 'loss/train': 6.093902318477631, 'loss/val': 6.103014087677002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4006/6000 [06:20<05:12,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing lora reset at update step 1001. Current lr is 1.2414075988794581e-06\n",
      "LoRA reset took 0.03s\n",
      "Performing optimizer reset at update step 1001. Current lr is 1.2414075988794581e-06\n",
      "Percent of optimizer states zeroed: 80.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 4202/6000 [06:38<06:30,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 1050, 'loss/train': 6.096440486907959, 'loss/val': 6.122192335128784}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 4290/6000 [06:46<02:24, 11.85it/s]"
     ]
    }
   ],
   "source": [
    "lora_model.train()\n",
    "\n",
    "train_losses = dict()\n",
    "val_losses = dict()\n",
    "last_losses = list()\n",
    "completed_steps = adjust_train_iters\n",
    "n_lora_restarts = 0\n",
    "n_optimizer_resets = 0\n",
    "\n",
    "for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=int((train_iters - adjust_train_iters) * accumulation_steps)\n",
    "    ):\n",
    "    output = lora_model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "    loss = output.loss\n",
    "    last_losses.append(loss.item())\n",
    "    loss /= accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    if step % accumulation_steps == 0:\n",
    "        accelerator.clip_grad_norm_(lora_model.parameters(), gradient_clipping)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    \n",
    "    if step % (eval_save_interval * accumulation_steps) == 0:\n",
    "        train_losses[completed_steps] = np.mean(last_losses)\n",
    "        val_losses[completed_steps] = evaluate(lora_model, val_dataloader, val_iters)\n",
    "        print(\n",
    "            {\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": train_losses[completed_steps],\n",
    "                \"loss/val\": val_losses[completed_steps],\n",
    "            }\n",
    "        )\n",
    "        last_losses = list()\n",
    "        lora_model.train()\n",
    "        accelerator.wait_for_everyone()\n",
    "    \n",
    "    if completed_steps >= train_iters:\n",
    "        accelerator.wait_for_everyone()\n",
    "        break\n",
    "\n",
    "    if step % accumulation_steps != 0:\n",
    "        continue\n",
    "\n",
    "    can_reset_relora = relora_steps is not None and completed_steps >= relora_steps\n",
    "\n",
    "    if can_reset_relora and completed_steps % relora_steps == 1:\n",
    "        _lora_reset_time = time.time()\n",
    "        print(f\"Performing lora reset at update step {completed_steps}. Current lr is {optimizer.param_groups[0]['lr']}\")\n",
    "        n_lora_restarts += 1\n",
    "\n",
    "        lora_model = lora_model.merge_and_unload()\n",
    "        lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "        lora_model = accelerator.prepare(lora_model)\n",
    "                \n",
    "        trainable_params = [p for p in lora_model.parameters() if p.requires_grad]\n",
    "    \n",
    "        optimizer.param_groups[0]['params'] = trainable_params\n",
    "        \n",
    "        _lora_reset_time = time.time() - _lora_reset_time\n",
    "        print(f\"LoRA reset took {_lora_reset_time:.2f}s\")\n",
    "\n",
    "        # scheduler should provide a new warmup after the reset\n",
    "        print(f\"Performing optimizer reset at update step {completed_steps}. Current lr is {optimizer.param_groups[0]['lr']}\")\n",
    "        n_optimizer_resets += 1\n",
    "\n",
    "        optimizer_reset(\n",
    "            optimizer,\n",
    "            reset_params=lora_params,\n",
    "            optimizer_state_keys=optimizer_state_keys,\n",
    "            reset_optimizer_on_relora=reset_optimizer_on_relora,\n",
    "            optimizer_random_pruning=0.0,\n",
    "            optimizer_magnitude_pruning=optimizer_magnitude_pruning,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
