{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:34.053242Z",
     "iopub.status.busy": "2024-06-01T22:17:34.052894Z",
     "iopub.status.idle": "2024-06-01T22:17:38.017994Z",
     "shell.execute_reply": "2024-06-01T22:17:38.017434Z",
     "shell.execute_reply.started": "2024-06-01T22:17:34.053222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:38.019676Z",
     "iopub.status.busy": "2024-06-01T22:17:38.019320Z",
     "iopub.status.idle": "2024-06-01T22:17:38.024892Z",
     "shell.execute_reply": "2024-06-01T22:17:38.024376Z",
     "shell.execute_reply.started": "2024-06-01T22:17:38.019655Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:38.025713Z",
     "iopub.status.busy": "2024-06-01T22:17:38.025547Z",
     "iopub.status.idle": "2024-06-01T22:17:38.029091Z",
     "shell.execute_reply": "2024-06-01T22:17:38.028622Z",
     "shell.execute_reply.started": "2024-06-01T22:17:38.025694Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "global_batch_size = 512\n",
    "accumulation_steps = global_batch_size / batch_size\n",
    "\n",
    "learning_rate = 1e-4\n",
    "betas = (0.9, 0.95)\n",
    "eps = 1e-8\n",
    "gradient_clipping = 1.0\n",
    "weight_decay = 0.1\n",
    "\n",
    "warmup_iters = 256\n",
    "\n",
    "train_iters = 2048\n",
    "\n",
    "model_name = \"EleutherAI/pythia-14m\"\n",
    "model_revision = \"step0\"\n",
    "\n",
    "dataset_path = \"allenai/c4\"\n",
    "dataset_name = \"realnewslike\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:38.029881Z",
     "iopub.status.busy": "2024-06-01T22:17:38.029724Z",
     "iopub.status.idle": "2024-06-01T22:17:49.956499Z",
     "shell.execute_reply": "2024-06-01T22:17:49.955949Z",
     "shell.execute_reply.started": "2024-06-01T22:17:38.029864Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7e9692760346d8a002484f8304cc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb9a472c4b84d4c834400002bbe809a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c267861e7674c6aae56e843c56690fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, dataset_name)\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:49.957535Z",
     "iopub.status.busy": "2024-06-01T22:17:49.957234Z",
     "iopub.status.idle": "2024-06-01T22:17:50.042717Z",
     "shell.execute_reply": "2024-06-01T22:17:50.042162Z",
     "shell.execute_reply.started": "2024-06-01T22:17:49.957516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select(range(65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:50.046295Z",
     "iopub.status.busy": "2024-06-01T22:17:50.045923Z",
     "iopub.status.idle": "2024-06-01T22:17:50.281842Z",
     "shell.execute_reply": "2024-06-01T22:17:50.281320Z",
     "shell.execute_reply.started": "2024-06-01T22:17:50.046275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, revision=model_revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:50.282655Z",
     "iopub.status.busy": "2024-06-01T22:17:50.282491Z",
     "iopub.status.idle": "2024-06-01T22:17:51.577320Z",
     "shell.execute_reply": "2024-06-01T22:17:51.576813Z",
     "shell.execute_reply.started": "2024-06-01T22:17:50.282637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 264339\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 55063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(data):\n",
    "    outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = list()\n",
    "    # deleting samples shorter than context_length tokens\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names, num_proc=60\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:51.578159Z",
     "iopub.status.busy": "2024-06-01T22:17:51.577985Z",
     "iopub.status.idle": "2024-06-01T22:17:51.580654Z",
     "shell.execute_reply": "2024-06-01T22:17:51.580224Z",
     "shell.execute_reply.started": "2024-06-01T22:17:51.578140Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "val_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:51.581362Z",
     "iopub.status.busy": "2024-06-01T22:17:51.581212Z",
     "iopub.status.idle": "2024-06-01T22:17:51.585249Z",
     "shell.execute_reply": "2024-06-01T22:17:51.584771Z",
     "shell.execute_reply.started": "2024-06-01T22:17:51.581345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              worker_init_fn=seed_worker,\n",
    "                              generator=g)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            worker_init_fn=seed_worker,\n",
    "                            generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:51.585980Z",
     "iopub.status.busy": "2024-06-01T22:17:51.585832Z",
     "iopub.status.idle": "2024-06-01T22:17:53.158078Z",
     "shell.execute_reply": "2024-06-01T22:17:53.157489Z",
     "shell.execute_reply.started": "2024-06-01T22:17:51.585964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                          revision=model_revision,\n",
    "                                                          # attn_implementation=\"flash_attention_2\",\n",
    "                                                          torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optim reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.159033Z",
     "iopub.status.busy": "2024-06-01T22:17:53.158860Z",
     "iopub.status.idle": "2024-06-01T22:17:53.165823Z",
     "shell.execute_reply": "2024-06-01T22:17:53.165321Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.159016Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimizer_reset(\n",
    "    optimizer,\n",
    "    *,\n",
    "    reset_params: list[torch.nn.Parameter],\n",
    "    optimizer_state_keys: list[str],\n",
    "    reset_optimizer_on_relora: bool,\n",
    "    optimizer_random_pruning: float,\n",
    "    optimizer_magnitude_pruning: float,\n",
    "):\n",
    "    \"\"\"\n",
    "        optimizer_state_keys: e.g., [\"exp_avg\", \"exp_avg_sq\"]\n",
    "    \"\"\"\n",
    "    n_reset_types = (\n",
    "        int(bool(reset_optimizer_on_relora))\n",
    "        + int(bool(optimizer_random_pruning))\n",
    "        + int(bool(optimizer_magnitude_pruning))\n",
    "    )\n",
    "    if n_reset_types != 1:\n",
    "        logger.warning(f\"Got {reset_optimizer_on_relora=}, {optimizer_random_pruning=}, \"\n",
    "                       f\"{optimizer_magnitude_pruning=}\")\n",
    "        raise ValueError(f\"Exactly one of reset_optimizer_on_relora, \"\n",
    "                         f\"optimizer_random_pruning, optimizer_magnitude_pruning must be True\")\n",
    "\n",
    "    # pruning_fn has to be inplace to work with ZeroRedundancyOptimizer\n",
    "    if reset_optimizer_on_relora:\n",
    "        logger.info(\"Resetting optimizer states to zeros\")\n",
    "        # looks like zeroing out breaks dictionary in the optimizer\n",
    "        # see full error below\n",
    "        pruning_fn = partial(random_pruning_, prune_ratio=0.999)\n",
    "    elif optimizer_random_pruning:\n",
    "        logger.info(f\"Performing random pruning of optimizer states. \"\n",
    "                    f\"Pruning {optimizer_random_pruning} percent\")\n",
    "        pruning_fn = partial(random_pruning_, prune_ratio=optimizer_random_pruning)\n",
    "    elif optimizer_magnitude_pruning:\n",
    "        logger.info(f\"Performing magnitude pruning of optimizer states. \"\n",
    "                    f\"Pruning {optimizer_magnitude_pruning} percent\")\n",
    "        pruning_fn = partial(magnitude_pruning_, prune_ratio=optimizer_magnitude_pruning)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown pruning type\")\n",
    "        \n",
    "    n_zeros = 0\n",
    "    n_total = 0\n",
    "\n",
    "    optimizer_state = optimizer.state\n",
    "    if isinstance(optimizer, ZeroRedundancyOptimizer):\n",
    "        optimizer_state = optimizer.optim.state\n",
    "\n",
    "    for p in reset_params:\n",
    "        param_state = optimizer_state[p]\n",
    "        if len(param_state) == 0: # no state for this param, happens for ZeRo optimizer\n",
    "            continue\n",
    "        for key in optimizer_state_keys:\n",
    "            pruning_fn(param_state[key])  # pruning fn has to be inplace to keep the same keys in the dict\n",
    "            n_total += param_state[key].numel()\n",
    "            n_zeros += torch.sum(param_state[key] == 0).item()\n",
    "\n",
    "    _zeroed = n_zeros / (1e-7 + n_total) * 100\n",
    "    logger.info(f\"Percent of optimizer states zeroed: {_zeroed:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLora Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:23:58.547595Z",
     "iopub.status.busy": "2024-06-01T22:23:58.546969Z",
     "iopub.status.idle": "2024-06-01T22:23:58.550531Z",
     "shell.execute_reply": "2024-06-01T22:23:58.550059Z",
     "shell.execute_reply.started": "2024-06-01T22:23:58.547573Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(r=128, \n",
    "                         target_modules=[\"query_key_value\", \"dense\",\n",
    "                                         \"dense_h_to_4h\", \"dense_4h_to_h\"], \n",
    "                         lora_dropout=0.1, \n",
    "                         lora_alpha=32)\n",
    "\n",
    "# By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and zeros for weight B \n",
    "# resulting in an identity transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.728178Z",
     "iopub.status.busy": "2024-06-01T22:17:53.728012Z",
     "iopub.status.idle": "2024-06-01T22:17:53.838533Z",
     "shell.execute_reply": "2024-06-01T22:17:53.837894Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.728161Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 15,640,576 || trainable%: 10.0563\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:25:35.068032Z",
     "iopub.status.busy": "2024-06-01T22:25:35.067426Z",
     "iopub.status.idle": "2024-06-01T22:25:35.072424Z",
     "shell.execute_reply": "2024-06-01T22:25:35.071845Z",
     "shell.execute_reply.started": "2024-06-01T22:25:35.068011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in lora_model.parameters():\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.839366Z",
     "iopub.status.busy": "2024-06-01T22:17:53.839202Z",
     "iopub.status.idle": "2024-06-01T22:17:53.843242Z",
     "shell.execute_reply": "2024-06-01T22:17:53.842770Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.839348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainable_params = [p for p in lora_model.parameters() if p.requires_grad]\n",
    "lora_params = [p for n, p in lora_model.named_parameters() if p.requires_grad and \"lora_\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.843991Z",
     "iopub.status.busy": "2024-06-01T22:17:53.843839Z",
     "iopub.status.idle": "2024-06-01T22:17:53.846225Z",
     "shell.execute_reply": "2024-06-01T22:17:53.845808Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.843975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.846937Z",
     "iopub.status.busy": "2024-06-01T22:17:53.846792Z",
     "iopub.status.idle": "2024-06-01T22:17:53.852668Z",
     "shell.execute_reply": "2024-06-01T22:17:53.852196Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.846920Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "optimizer_state_keys = [\"exp_avg\", \"exp_avg_sq\"]\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_iters, num_training_steps=train_iters, num_cycles=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.853441Z",
     "iopub.status.busy": "2024-06-01T22:17:53.853286Z",
     "iopub.status.idle": "2024-06-01T22:17:53.855825Z",
     "shell.execute_reply": "2024-06-01T22:17:53.855401Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.853425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "update_step = 0\n",
    "tokens_seen = 0\n",
    "tokens_seen_before = 0\n",
    "n_lora_restarts = 0\n",
    "n_optimizer_resets = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.856544Z",
     "iopub.status.busy": "2024-06-01T22:17:53.856399Z",
     "iopub.status.idle": "2024-06-01T22:17:53.858753Z",
     "shell.execute_reply": "2024-06-01T22:17:53.858336Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.856529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "relora_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.859448Z",
     "iopub.status.busy": "2024-06-01T22:17:53.859302Z",
     "iopub.status.idle": "2024-06-01T22:17:53.861890Z",
     "shell.execute_reply": "2024-06-01T22:17:53.861457Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.859432Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycle_length = scheduler.lr_lambdas[0].keywords['num_training_steps'] // scheduler.lr_lambdas[0].keywords['num_cycles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.862620Z",
     "iopub.status.busy": "2024-06-01T22:17:53.862472Z",
     "iopub.status.idle": "2024-06-01T22:17:53.865621Z",
     "shell.execute_reply": "2024-06-01T22:17:53.865204Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.862604Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(inputs, logits):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.868008Z",
     "iopub.status.busy": "2024-06-01T22:17:53.867729Z",
     "iopub.status.idle": "2024-06-01T22:17:53.869984Z",
     "shell.execute_reply": "2024-06-01T22:17:53.869550Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.867990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:53.870753Z",
     "iopub.status.busy": "2024-06-01T22:17:53.870596Z",
     "iopub.status.idle": "2024-06-01T22:17:54.067315Z",
     "shell.execute_reply": "2024-06-01T22:17:54.066838Z",
     "shell.execute_reply.started": "2024-06-01T22:17:53.870736Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "lora_model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    lora_model, optimizer, train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:17:54.068091Z",
     "iopub.status.busy": "2024-06-01T22:17:54.067938Z",
     "iopub.status.idle": "2024-06-01T22:17:56.495146Z",
     "shell.execute_reply": "2024-06-01T22:17:56.494389Z",
     "shell.execute_reply.started": "2024-06-01T22:17:54.068075Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2048 [00:02<22:44,  1.50it/s]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2269\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2267\u001b[0m             \u001b[38;5;66;03m# Set is_xla_gradients_synced to True to avoid all-reduce twice in the AcceleratedOptimizer step.\u001b[39;00m\n\u001b[1;32m   2268\u001b[0m             acc_opt\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2219\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2218\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:307\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    304\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    305\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 307\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:229\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "completed_steps = 0\n",
    "\n",
    "for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=train_iters\n",
    "    ):\n",
    "    \n",
    "    logits = model(batch[\"input_ids\"]).logits\n",
    "    loss = calculate_loss(batch[\"input_ids\"], logits)\n",
    "    loss = loss / accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % accumulation_steps == 0:\n",
    "        accelerator.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        accelerator.print(\n",
    "            {\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": loss.item() * accumulation_steps,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    can_reset_relora = relora_steps is not None and step // accumulation_steps >= relora_steps\n",
    "\n",
    "    if can_reset_relora and completed_steps % relora_steps == 1:\n",
    "        _lora_reset_time = time.time()\n",
    "        logger.info(f\"Performing lora reset at update step {completed_steps}. Current lr is {optimizer.param_groups[0]['lr']}\")\n",
    "        n_lora_restarts += 1\n",
    "\n",
    "        lora_model = lora_model.merge_and_unload()\n",
    "        lora_model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        for param in lora_model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = param.data.float()\n",
    "\n",
    "        _lora_reset_time = time.time() - _lora_reset_time\n",
    "        logger.info(f\"LoRA reset took {_lora_reset_time:.2f}s\")\n",
    "\n",
    "    can_reset_optimizer = relora_steps is not None and step // accumulation_steps >= cycle_length\n",
    "\n",
    "    if can_reset_optimizer and (completed_steps - scheduler_start_step) % cycle_length == 1:\n",
    "        # scheduler should provide a new warmup after the reset\n",
    "        logger.info(f\"Performing optimizer reset at update step {update_step}. Current lr is {optimizer.param_groups[0]['lr']}\")\n",
    "        n_optimizer_resets += 1\n",
    "\n",
    "        training_utils.optimizer_reset(\n",
    "            optimizer,\n",
    "            reset_params=lora_params,\n",
    "            optimizer_state_keys=optimizer_state_keys,\n",
    "            reset_optimizer_on_relora=args.reset_optimizer_on_relora,\n",
    "            optimizer_random_pruning=args.optimizer_random_pruning,\n",
    "            optimizer_magnitude_pruning=args.optimizer_magnitude_pruning,\n",
    "        )\n",
    "    # ##############################\n",
    "\n",
    "    if can_reset_optimizer and (completed_steps - scheduler_start_step) % cycle_length == 2:\n",
    "        logger.info(f\"First step after optimizer reset lr is {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    lr = optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-rank Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_iters, num_training_steps=train_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(inputs, logits):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 102/2048 [00:10<03:00, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 25, 'loss/train': 10.98653507232666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 202/2048 [00:20<02:50, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 50, 'loss/train': 10.87724781036377}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 302/2048 [00:30<02:41, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 75, 'loss/train': 10.707901954650879}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 402/2048 [00:40<02:32, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 100, 'loss/train': 10.520821571350098}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 502/2048 [00:50<02:23, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 125, 'loss/train': 10.398367881774902}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 602/2048 [01:00<02:14, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 150, 'loss/train': 10.303812026977539}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 702/2048 [01:11<02:05, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 175, 'loss/train': 10.22122859954834}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 802/2048 [01:21<01:56, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 200, 'loss/train': 10.120756149291992}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 902/2048 [01:31<01:46, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 225, 'loss/train': 10.018835067749023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 1002/2048 [01:41<01:38, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 250, 'loss/train': 9.94930362701416}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 1102/2048 [01:51<01:27, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 275, 'loss/train': 9.759871482849121}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 1202/2048 [02:01<01:18, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 300, 'loss/train': 9.670370101928711}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 1302/2048 [02:11<01:09, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 325, 'loss/train': 9.555130958557129}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1402/2048 [02:21<01:00, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 350, 'loss/train': 9.479826927185059}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1502/2048 [02:31<00:50, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 375, 'loss/train': 9.386810302734375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1602/2048 [02:41<00:41, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 400, 'loss/train': 9.324230194091797}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1702/2048 [02:51<00:32, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 425, 'loss/train': 9.19173526763916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1802/2048 [03:01<00:23, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 450, 'loss/train': 9.118826866149902}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1902/2048 [03:12<00:13, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 475, 'loss/train': 9.068615913391113}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 2002/2048 [03:22<00:04, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 500, 'loss/train': 8.984365463256836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2066it [03:28,  9.90it/s]                          \n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "completed_steps = 0\n",
    "\n",
    "for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=train_iters\n",
    "    ):\n",
    "    \n",
    "    logits = model(batch[\"input_ids\"]).logits\n",
    "    loss = calculate_loss(batch[\"input_ids\"], logits)\n",
    "    loss = loss / accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % accumulation_steps == 0:\n",
    "        accelerator.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        accelerator.print(\n",
    "            {\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": loss.item() * accumulation_steps,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
