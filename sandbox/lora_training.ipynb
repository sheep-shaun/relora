{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_path = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:01.376482Z",
     "iopub.status.busy": "2024-06-02T21:01:01.375920Z",
     "iopub.status.idle": "2024-06-02T21:01:01.381926Z",
     "shell.execute_reply": "2024-06-02T21:01:01.381428Z",
     "shell.execute_reply.started": "2024-06-02T21:01:01.376461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:01.383018Z",
     "iopub.status.busy": "2024-06-02T21:01:01.382569Z",
     "iopub.status.idle": "2024-06-02T21:01:01.386103Z",
     "shell.execute_reply": "2024-06-02T21:01:01.385658Z",
     "shell.execute_reply.started": "2024-06-02T21:01:01.382999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_length = 128\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "global_batch_size = 512\n",
    "accumulation_steps = global_batch_size // batch_size\n",
    "\n",
    "learning_rate = 3e-4\n",
    "betas = (0.9, 0.95)\n",
    "eps = 1e-8\n",
    "gradient_clipping = 1.0\n",
    "weight_decay = 0.1\n",
    "\n",
    "warmup_iters = 256\n",
    "\n",
    "train_iters = 2048\n",
    "eval_save_interval = 50\n",
    "val_iters = 20\n",
    "\n",
    "lora_rank = 128\n",
    "lora_dropout = 0.1\n",
    "lora_alpha = 32\n",
    "relora_steps = 200\n",
    "reset_optimizer_on_relora = False\n",
    "optimizer_magnitude_pruning = 0.8\n",
    "\n",
    "model_name = \"EleutherAI/pythia-14m\"\n",
    "model_revision = \"step0\"\n",
    "\n",
    "dataset_path = \"allenai/c4\"\n",
    "dataset_name = \"realnewslike\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:01.387560Z",
     "iopub.status.busy": "2024-06-02T21:01:01.387394Z",
     "iopub.status.idle": "2024-06-02T21:01:12.888299Z",
     "shell.execute_reply": "2024-06-02T21:01:12.887759Z",
     "shell.execute_reply.started": "2024-06-02T21:01:01.387543Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6660feb9322f47d8ac911b66c77a284e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a310ac2e9bd94bd59b7d03efa5ee2f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55000c3831148fbaa5399ba0239f49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, dataset_name)\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:01:12.889263Z",
     "iopub.status.busy": "2024-06-02T21:01:12.888985Z",
     "iopub.status.idle": "2024-06-02T21:01:12.891906Z",
     "shell.execute_reply": "2024-06-02T21:01:12.891457Z",
     "shell.execute_reply.started": "2024-06-02T21:01:12.889243Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, revision=model_revision)\n",
    "\n",
    "def tokenize(data):\n",
    "    outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = list()\n",
    "    # deleting samples shorter than context_length tokens\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:07:44.222661Z",
     "iopub.status.busy": "2024-06-02T21:07:44.222486Z",
     "iopub.status.idle": "2024-06-02T21:07:44.225462Z",
     "shell.execute_reply": "2024-06-02T21:07:44.225034Z",
     "shell.execute_reply.started": "2024-06-02T21:07:44.222641Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da7e505196a42b6bcff3e08dc461a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(data_path, \"train_dataset\")):\n",
    "    train_dataset = Dataset.load_from_disk(os.path.join(data_path, \"train_dataset\"))\n",
    "else:\n",
    "    train_dataset = dataset[\"train\"].map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    train_dataset.save_to_disk(os.path.join(data_path, \"train_dataset\"))\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(data_path, \"val_dataset\")):\n",
    "    val_dataset = Dataset.load_from_disk(os.path.join(data_path, \"val_dataset\"))\n",
    "else:\n",
    "    val_dataset = dataset[\"validation\"].map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"validation\"].column_names\n",
    "    )\n",
    "    val_dataset.save_to_disk(os.path.join(data_path, \"val_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.select(range(len(val_dataset) // (batch_size * val_iters) * (batch_size * val_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:07:56.497721Z",
     "iopub.status.busy": "2024-06-02T21:07:56.497152Z",
     "iopub.status.idle": "2024-06-02T21:07:56.501566Z",
     "shell.execute_reply": "2024-06-02T21:07:56.501092Z",
     "shell.execute_reply.started": "2024-06-02T21:07:56.497700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              worker_init_fn=seed_worker,\n",
    "                              generator=g)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            worker_init_fn=seed_worker,\n",
    "                            generator=g,\n",
    "                            drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optim reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:42:26.052579Z",
     "iopub.status.busy": "2024-06-02T21:42:26.051926Z",
     "iopub.status.idle": "2024-06-02T21:42:26.056774Z",
     "shell.execute_reply": "2024-06-02T21:42:26.056268Z",
     "shell.execute_reply.started": "2024-06-02T21:42:26.052554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def random_pruning_(tensor, prune_ratio):\n",
    "    \"\"\"\n",
    "    Performs random pruning dimensionality reduction **inplace**.\n",
    "    Only reduces the inner dimensionality, does not affect the shape of the tensor\n",
    "    \"\"\"\n",
    "    random_pruning_mask = torch.rand_like(tensor) > prune_ratio\n",
    "    tensor.mul_(random_pruning_mask)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def magnitude_pruning_(tensor, prune_ratio):\n",
    "    \"\"\"\n",
    "    Performs magnitude pruning dimensionality reduction **inplace**.\n",
    "    Only reduces the inner dimensionality, does not affect the shape of the tensor\n",
    "    \"\"\"\n",
    "    tensor_magnitude = torch.abs(tensor)\n",
    "    threshold = torch.quantile(tensor_magnitude.flatten().to(dtype=torch.float32), prune_ratio).to(dtype=tensor.dtype)\n",
    "\n",
    "    mask = tensor_magnitude > threshold\n",
    "    tensor.mul_(mask.to(dtype=tensor.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:56:45.192101Z",
     "iopub.status.busy": "2024-06-02T21:56:45.191349Z",
     "iopub.status.idle": "2024-06-02T21:56:45.198582Z",
     "shell.execute_reply": "2024-06-02T21:56:45.198063Z",
     "shell.execute_reply.started": "2024-06-02T21:56:45.192074Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimizer_reset(\n",
    "    optimizer,\n",
    "    *,\n",
    "    reset_params: list[torch.nn.Parameter],\n",
    "    optimizer_state_keys: list[str],\n",
    "    reset_optimizer_on_relora: bool,\n",
    "    optimizer_random_pruning: float,\n",
    "    optimizer_magnitude_pruning: float,\n",
    "):\n",
    "    \"\"\"\n",
    "        optimizer_state_keys: e.g., [\"exp_avg\", \"exp_avg_sq\"]\n",
    "    \"\"\"\n",
    "    n_reset_types = (\n",
    "        int(bool(reset_optimizer_on_relora))\n",
    "        + int(bool(optimizer_random_pruning))\n",
    "        + int(bool(optimizer_magnitude_pruning))\n",
    "    )\n",
    "    if n_reset_types != 1:\n",
    "        # logger.warning(f\"Got {reset_optimizer_on_relora=}, {optimizer_random_pruning=}, \"\n",
    "        #                f\"{optimizer_magnitude_pruning=}\")\n",
    "        raise ValueError(f\"Exactly one of reset_optimizer_on_relora, \"\n",
    "                         f\"optimizer_random_pruning, optimizer_magnitude_pruning must be True\")\n",
    "\n",
    "    # pruning_fn has to be inplace to work with ZeroRedundancyOptimizer\n",
    "    if reset_optimizer_on_relora:\n",
    "        pruning_fn = partial(random_pruning_, prune_ratio=0.999)\n",
    "    elif optimizer_random_pruning:\n",
    "        pruning_fn = partial(random_pruning_, prune_ratio=optimizer_random_pruning)\n",
    "    elif optimizer_magnitude_pruning:\n",
    "        pruning_fn = partial(magnitude_pruning_, prune_ratio=optimizer_magnitude_pruning)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown pruning type\")\n",
    "        \n",
    "    n_zeros = 0\n",
    "    n_total = 0\n",
    "\n",
    "    optimizer_state = optimizer.state\n",
    "    if isinstance(optimizer, ZeroRedundancyOptimizer):\n",
    "        optimizer_state = optimizer.optim.state\n",
    "\n",
    "    for p in reset_params:\n",
    "        param_state = optimizer_state[p]\n",
    "        if len(param_state) == 0: # no state for this param, happens for ZeRo optimizer\n",
    "            continue\n",
    "        for key in optimizer_state_keys:\n",
    "            pruning_fn(param_state[key])  # pruning fn has to be inplace to keep the same keys in the dict\n",
    "            n_total += param_state[key].numel()\n",
    "            n_zeros += torch.sum(param_state[key] == 0).item()\n",
    "\n",
    "    _zeroed = n_zeros / (1e-7 + n_total) * 100\n",
    "    print(f\"Percent of optimizer states zeroed: {_zeroed:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLora Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:56:45.779339Z",
     "iopub.status.busy": "2024-06-02T21:56:45.778937Z",
     "iopub.status.idle": "2024-06-02T21:56:45.782142Z",
     "shell.execute_reply": "2024-06-02T21:56:45.781664Z",
     "shell.execute_reply.started": "2024-06-02T21:56:45.779319Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=lora_rank, \n",
    "                         target_modules=[\"query_key_value\", \"dense\",\n",
    "                                         \"dense_h_to_4h\", \"dense_4h_to_h\"], \n",
    "                         lora_dropout=lora_dropout, \n",
    "                         lora_alpha=lora_alpha)\n",
    "\n",
    "# By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and zeros for weight B \n",
    "# resulting in an identity transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:31.507159Z",
     "iopub.status.busy": "2024-06-02T21:58:31.506522Z",
     "iopub.status.idle": "2024-06-02T21:58:32.054278Z",
     "shell.execute_reply": "2024-06-02T21:58:32.053731Z",
     "shell.execute_reply.started": "2024-06-02T21:58:31.507134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in GPTNeoXForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in GPTNeoXModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                          revision=model_revision,\n",
    "                                                          attn_implementation=\"flash_attention_2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:32.055442Z",
     "iopub.status.busy": "2024-06-02T21:58:32.055272Z",
     "iopub.status.idle": "2024-06-02T21:58:32.092967Z",
     "shell.execute_reply": "2024-06-02T21:58:32.092461Z",
     "shell.execute_reply.started": "2024-06-02T21:58:32.055425Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 15,640,576 || trainable%: 10.0563\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:32.093781Z",
     "iopub.status.busy": "2024-06-02T21:58:32.093618Z",
     "iopub.status.idle": "2024-06-02T21:58:32.097852Z",
     "shell.execute_reply": "2024-06-02T21:58:32.097393Z",
     "shell.execute_reply.started": "2024-06-02T21:58:32.093763Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainable_params = [p for p in lora_model.parameters() if p.requires_grad]\n",
    "lora_params = [p for n, p in lora_model.named_parameters() if p.requires_grad and \"lora_\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:34.132990Z",
     "iopub.status.busy": "2024-06-02T21:58:34.132535Z",
     "iopub.status.idle": "2024-06-02T21:58:34.136419Z",
     "shell.execute_reply": "2024-06-02T21:58:34.135933Z",
     "shell.execute_reply.started": "2024-06-02T21:58:34.132971Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "optimizer_state_keys = [\"exp_avg\", \"exp_avg_sq\"]\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_iters, num_training_steps=train_iters, num_cycles=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:44.660204Z",
     "iopub.status.busy": "2024-06-02T21:58:44.659648Z",
     "iopub.status.idle": "2024-06-02T21:58:44.662780Z",
     "shell.execute_reply": "2024-06-02T21:58:44.662342Z",
     "shell.execute_reply.started": "2024-06-02T21:58:44.660185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycle_length = scheduler.lr_lambdas[0].keywords['num_training_steps'] // scheduler.lr_lambdas[0].keywords['num_cycles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:45.350100Z",
     "iopub.status.busy": "2024-06-02T21:58:45.349644Z",
     "iopub.status.idle": "2024-06-02T21:58:45.358667Z",
     "shell.execute_reply": "2024-06-02T21:58:45.358189Z",
     "shell.execute_reply.started": "2024-06-02T21:58:45.350081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "lora_model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "    lora_model, optimizer, train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, val_iters):\n",
    "    model.eval()\n",
    "    val_losses = list()\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            val_losses.append(outputs.loss.item())\n",
    "        if step + 1 >= val_iters:\n",
    "            break\n",
    "    val_loss = np.mean(val_losses)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T21:58:49.261706Z",
     "iopub.status.busy": "2024-06-02T21:58:49.261283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/8192 [00:04<39:46,  3.43it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m lora_model(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[0;32m---> 15\u001b[0m last_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lora_model.train()\n",
    "\n",
    "train_losses = dict()\n",
    "val_losses = dict()\n",
    "last_losses = list()\n",
    "completed_steps = 0\n",
    "n_lora_restarts = 0\n",
    "n_optimizer_resets = 0\n",
    "\n",
    "for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=int(train_iters * accumulation_steps)\n",
    "    ):\n",
    "    output = lora_model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "    loss = output.loss\n",
    "    last_losses.append(loss.item())\n",
    "    loss /= accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    if step % accumulation_steps == 0:\n",
    "        accelerator.clip_grad_norm_(lora_model.parameters(), gradient_clipping)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    \n",
    "    if step % (eval_save_interval * accumulation_steps) == 0:\n",
    "        train_losses[completed_steps] = np.mean(last_losses)\n",
    "        val_losses[completed_steps] = evaluate(lora_model, val_dataloader, val_iters)\n",
    "        print(\n",
    "            {\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": train_losses[completed_steps],\n",
    "                \"loss/val\": val_losses[completed_steps],\n",
    "            }\n",
    "        )\n",
    "        last_losses = list()\n",
    "        lora_model.train()\n",
    "        accelerator.wait_for_everyone()\n",
    "    \n",
    "    if completed_steps >= train_iters:\n",
    "        train_losses[completed_steps] = np.mean(last_losses)\n",
    "        val_losses[completed_steps] = evaluate(lora_model, val_dataloader, val_iters)\n",
    "        print(\n",
    "            {\n",
    "                \"steps\": completed_steps,\n",
    "                \"loss/train\": train_losses[completed_steps],\n",
    "                \"loss/val\": val_losses[completed_steps],\n",
    "            }\n",
    "        )\n",
    "        last_losses = list()\n",
    "        lora_model.train()\n",
    "        accelerator.wait_for_everyone()\n",
    "        break\n",
    "\n",
    "    if step % accumulation_steps != 0:\n",
    "        continue\n",
    "\n",
    "    can_reset_relora = relora_steps is not None and completed_steps >= relora_steps\n",
    "\n",
    "    if can_reset_relora and completed_steps % relora_steps == 1:\n",
    "        _lora_reset_time = time.time()\n",
    "        print(f\"Performing lora reset at update step {completed_steps}. Current lr is {optimizer.param_groups[0]['lr']}\")\n",
    "        n_lora_restarts += 1\n",
    "\n",
    "        lora_model = lora_model.merge_and_unload()\n",
    "        lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "        lora_model = accelerator.prepare(lora_model)\n",
    "                \n",
    "        trainable_params = [p for p in lora_model.parameters() if p.requires_grad]\n",
    "    \n",
    "        optimizer.param_groups[0]['params'] = trainable_params\n",
    "        \n",
    "        _lora_reset_time = time.time() - _lora_reset_time\n",
    "        print(f\"LoRA reset took {_lora_reset_time:.2f}s\")\n",
    "\n",
    "    can_reset_optimizer = relora_steps is not None and completed_steps >= cycle_length\n",
    "\n",
    "    if can_reset_optimizer and (completed_steps - 0) % cycle_length == 1:\n",
    "        # scheduler should provide a new warmup after the reset\n",
    "        print(f\"Performing optimizer reset at update step {completed_steps}. Current lr is {optimizer.param_groups[0]['lr']}\")\n",
    "        n_optimizer_resets += 1\n",
    "\n",
    "        optimizer_reset(\n",
    "            optimizer,\n",
    "            reset_params=lora_params,\n",
    "            optimizer_state_keys=optimizer_state_keys,\n",
    "            reset_optimizer_on_relora=reset_optimizer_on_relora,\n",
    "            optimizer_random_pruning=0.0,\n",
    "            optimizer_magnitude_pruning=optimizer_magnitude_pruning,\n",
    "        )\n",
    "\n",
    "    if can_reset_optimizer and (completed_steps - 0) % cycle_length == 2:\n",
    "        print(f\"First step after optimizer reset lr is {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
